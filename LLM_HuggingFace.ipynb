{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3nOqt0NjcWD"
      },
      "source": [
        "\n",
        "#Dùng LLM có sẳn của Hugging face\n",
        "\n",
        "-Model có chức năng tổng hợp câu (tóm tắt văn bản)\n",
        "-Trả lời câu hỏi\n",
        "-Đoán từ tiếp theo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoDSGUQ2dxcU"
      },
      "outputs": [],
      "source": [
        "#import locale\n",
        "locale.getpreferredencodding = lambda: \"UTF-8\"\n",
        "\n",
        "#bị lỗi khi cài đặt bằng lệnh pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fThx51NBeC0U"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers torch accelerate #cài transformers và bộ tăng tốc cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXUj1G3JedHT",
        "outputId": "57052eb2-3e43-4aca-8f4e-49f426ea9e96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig\n",
        "\n",
        "# Check nếu có GPU thì dùng, không có thì fallback về CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "# Config\n",
        "config = AutoConfig.from_pretrained(\"google/flan-t5-base\", trust_remote_code=True)\n",
        "config.temperature = 0.1\n",
        "config.max_length = 300\n",
        "config.eos_token_id = tokenizer.eos_token_id\n",
        "config.pad_token_id = tokenizer.pad_token_id\n",
        "config.do_sample = True\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    \"google/flan-t5-base\",\n",
        "    config=config\n",
        ").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM9fuwj4hhwe",
        "outputId": "da0ba62a-6d94-4291-847e-53a4273d868c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1737: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> The chatbot uses natural language processing to create human-like dialogue.</s>\n"
          ]
        }
      ],
      "source": [
        "input_text = \"summary this text: ChatGPT is an artificial intelligence (AI) chatbot that uses natural language processing to create humanlike conversational dialogue. The language model\"\n",
        "\n",
        "input_ids = tokenizer(input_text,return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
        "\n",
        "#Lấy output từ model\n",
        "\n",
        "outputs = model.generate(input_ids)\n",
        "\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkAR0JygjxKu",
        "outputId": "b7ec04e0-51f9-48cc-93c9-d9c688602032"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> Donald Trump</s>\n"
          ]
        }
      ],
      "source": [
        "input_text = \"Answer the following question: Can you tell me who is the president of US?\"\n",
        "input_ids = tokenizer(input_text,return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
        "\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saSCc026klZ3",
        "outputId": "b9852b2b-5afb-4fe6-8079-107bca1e8ff1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad> ice cream</s>\n"
          ]
        }
      ],
      "source": [
        "input_text = \"I am hungry, i want to eat some\"\n",
        "input_ids = tokenizer(input_text,return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
        "\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aI7HIMmkwBS",
        "outputId": "9601e653-7d41-4faf-f63c-fa50259ce82f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status Code: 404\n",
            "Response Text: Not Found\n",
            "Error decoding JSON: Expecting value: line 1 column 1 (char 0)\n",
            "Output: None\n"
          ]
        }
      ],
      "source": [
        "#Đã chặn API rồi\n",
        "\n",
        "import requests\n",
        "\n",
        "API_TOKEN =\"\"\n",
        "API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-base\"\n",
        "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
        "\n",
        "def query(payload):\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    print(\"Status Code:\", response.status_code)\n",
        "    print(\"Response Text:\", response.text)  # Xem API trả về gì\n",
        "    try:\n",
        "        return response.json()\n",
        "    except Exception as e:\n",
        "        print(\"Error decoding JSON:\", e)\n",
        "        return None\n",
        "\n",
        "output = query({\n",
        "    \"inputs\": \"The answer to the universe is\",\n",
        "})\n",
        "\n",
        "print(\"Output:\", output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbgGQJfN4xWX"
      },
      "source": [
        "#Đối với OpenAPI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMjVhl5t4z3Y",
        "outputId": "ff8b029f-224e-4b68-82b9-36243853c1e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.93.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (4.14.0)\n",
            "Collecting typing-extensions\n",
            "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typing-extensions\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.14.0\n",
            "    Uninstalling typing_extensions-4.14.0:\n",
            "      Successfully uninstalled typing_extensions-4.14.0\n",
            "Successfully installed typing-extensions-4.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai --upgrade\n",
        "!pip install typing-extensions --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8r2vSse5ADx"
      },
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hmdEA5d5GcD"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    api_key =\"\"\n",
        ")\n",
        "\n",
        "#trả phí"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
